import os
import json
import re
from bs4 import BeautifulSoup

# –£–∫–∞–∂–∏ —Ç–æ—á–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
input_file = "/content/–ü—Ä–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—å–Ω–∏–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç - –ó–∞–∫–æ–Ω ‚Ññ 2344-III –≤—ñ–¥ 05.04.2001 - d81073-20241115.htm"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTML –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON
def parse_law_html(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        soup = BeautifulSoup(file, "html.parser")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–∞ –∏–∑ <title>
    title = soup.find("title").text.strip() if soup.find("title") else "–ë–µ–∑ –Ω–∞–∑–≤–∏"

    # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –∑–∞–∫–æ–Ω–∞ –∏–∑ <div id="article">
    article_div = soup.find("div", {"id": "article"})
    if not article_div:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω <div id='article'>. –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏ –¥–∞—Ç—É –∑–∞–∫–æ–Ω–∞
    law_number = "–ù–µ–≤—ñ–¥–æ–º–∏–π –Ω–æ–º–µ—Ä"
    law_date = "–ù–µ–≤—ñ–¥–æ–º–∞ –¥–∞—Ç–∞"
    match = re.search(r"–≤—ñ–¥ (\d{2}\.\d{2}\.\d{4}) ‚Ññ ([\dIVXLCDM-]+)", title)
    if match:
        law_date = match.group(1)
        law_number = match.group(2)

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
    def extract_list_items(ul_or_ol):
        items = []
        for li in ul_or_ol.find_all("li", recursive=False):  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
            sub_list = li.find(["ul", "ol"])  # –ò—â–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            if sub_list:
                items.append({
                    "text": li.get_text(strip=True),
                    "sub_list": extract_list_items(sub_list)  # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤
                })
            else:
                items.append(li.get_text(strip=True))
        return items

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    content = []
    for element in article_div.find_all(["h1", "h2", "h3", "p", "ul", "ol"]):
        text = element.get_text(strip=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if element.name in ["h1", "h2", "h3"]:
            content.append({"type": "heading", "level": int(element.name[1]), "text": text})

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—å–∏ –∏ –≥–ª–∞–≤—ã
        elif element.name == "p" and text:
            if re.match(r"^–°—Ç–∞—Ç—Ç—è \d+", text):
                content.append({"type": "article", "text": text})
            elif re.match(r"^–†–æ–∑–¥—ñ–ª \d+", text):
                content.append({"type": "heading", "level": 1, "text": text})  # –†–∞–∑–¥–µ–ª
            elif re.match(r"^–ì–ª–∞–≤–∞ \d+", text):
                content.append({"type": "heading", "level": 2, "text": text})  # –ì–ª–∞–≤–∞
            else:
                content.append({"type": "paragraph", "text": text})

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–∫–∏
        elif element.name in ["ul", "ol"]:
            list_type = "unordered" if element.name == "ul" else "ordered"
            list_items = extract_list_items(element)
            content.append({"type": "list", "list_type": list_type, "items": list_items})

    # –°–æ–∑–¥–∞–µ–º JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É
    law_data = {
        "title": title,
        "law_number": law_number,
        "law_date": law_date,
        "content": content
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    json_filename = os.path.splitext(os.path.basename(file_path))[0] + ".json"
    output_path = os.path.join("/content/", json_filename)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    with open(output_path, "w", encoding="utf-8") as json_file:
        json.dump(law_data, json_file, ensure_ascii=False, indent=4)
    
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path} ‚Üí {json_filename}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
parse_law_html(input_file)

print("üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! JSON-—Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ /content/")
